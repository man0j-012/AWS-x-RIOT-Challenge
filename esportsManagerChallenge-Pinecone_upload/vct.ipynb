{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import gzip\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import json\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "import re\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File download and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the S3 client with anonymous (unsigned) access\n",
    "# s3_client = boto3.client('s3',\n",
    "#                          region_name='us-west-2',\n",
    "#                          config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "# # Define the original bucket name\n",
    "# original_bucket = 'vcthackathon-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def list_all_files_and_folders(bucket):\n",
    "#     paginator = s3_client.get_paginator('list_objects_v2')\n",
    "#     page_iterator = paginator.paginate(Bucket=bucket)\n",
    "    \n",
    "#     all_files = []\n",
    "    \n",
    "#     # Iterate over each page of results\n",
    "#     for page in page_iterator:\n",
    "#         # List all files (objects)\n",
    "#         if 'Contents' in page:\n",
    "#             for obj in page['Contents']:\n",
    "#                 all_files.append(obj['Key'])\n",
    "    \n",
    "#     return all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sanitize_filename(s3_key):\n",
    "#     return s3_key.replace(':', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def download_s3_file(bucket, s3_key, local_path):\n",
    "#     sanitized_path = sanitize_filename(local_path)\n",
    "    \n",
    "#     if not os.path.exists(os.path.dirname(sanitized_path)):\n",
    "#         os.makedirs(os.path.dirname(sanitized_path))\n",
    "    \n",
    "#     s3_client.download_file(bucket, s3_key, sanitized_path)\n",
    "#     print(f\"Downloaded {s3_key} to {sanitized_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def unzip_file(file_path):\n",
    "#     with gzip.open(file_path, 'rb') as f_in:\n",
    "#         output_file_path = file_path[:-3]  # Remove .gz extension for output file\n",
    "#         with open(output_file_path, 'wb') as f_out:\n",
    "#             shutil.copyfileobj(f_in, f_out)\n",
    "#         print(f\"Unzipped {file_path} to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def download_and_unzip_files(bucket):\n",
    "#     # List all files in the bucket\n",
    "#     all_files = list_all_files_and_folders(bucket)\n",
    "    \n",
    "#     print(f\"Total files found: {len(all_files)}\")\n",
    "\n",
    "#     # Iterate and filter files\n",
    "#     for s3_key in all_files:\n",
    "#         # If it's a file in the /games folder and the year is 2024, download it\n",
    "#         if \"/games/\" in s3_key and \"/2024/\" in s3_key:\n",
    "#             local_path = os.path.join(os.getcwd(), s3_key)\n",
    "#             download_s3_file(bucket, s3_key, local_path)\n",
    "#             unzip_file(sanitize_filename(local_path))\n",
    "        \n",
    "#         # Otherwise, download all non-games files\n",
    "#         elif \"/games/\" not in s3_key:\n",
    "#             local_path = os.path.join(os.getcwd(), s3_key)\n",
    "#             download_s3_file(bucket, s3_key, local_path)\n",
    "#             if local_path.endswith('.gz'):\n",
    "#                 unzip_file(sanitize_filename(local_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage to download and unzip the filtered files\n",
    "# download_and_unzip_files(original_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_gz_files(directory):\n",
    "#     for root, dirs, files in os.walk(directory):\n",
    "#         for file in files:\n",
    "#             if file.endswith('.gz'):\n",
    "#                 file_path = os.path.join(root, file)\n",
    "#                 try:\n",
    "#                     os.remove(file_path)\n",
    "#                     print(f\"Removed: {file_path}\")\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Failed to remove {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory_path = r\"D:\\University of Illinois Chicago\\Classes\\Hackathon\\esportsManagerChallenge\\data\"\n",
    "# remove_gz_files(directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAME_CHANGERS_ESPORTS_PATH = r\"D:\\University of Illinois Chicago\\Classes\\Hackathon\\esportsManagerChallenge\\data\\game-changers\\esports-data\"\n",
    "VCT_CHALLENGERS_ESPORTS_PATH = r\"D:\\University of Illinois Chicago\\Classes\\Hackathon\\esportsManagerChallenge\\data\\vct-challengers\\esports-data\"\n",
    "VCT_INTERNATIONAL_ESPORTS_PATH = r\"D:\\University of Illinois Chicago\\Classes\\Hackathon\\esportsManagerChallenge\\data\\vct-international\\esports-data\"\n",
    "GAME_CHANGERS_GAME_DATA_PATH = r\"D:\\University of Illinois Chicago\\Classes\\Hackathon\\esportsManagerChallenge\\data\\game-changers\\games\\2024\"\n",
    "VCT_CHALLENGERS_GAME_DATA_PATH = r\"D:\\University of Illinois Chicago\\Classes\\Hackathon\\esportsManagerChallenge\\data\\vct-challengers\\games\\2024\"\n",
    "VCT_INTERNATIONAL_GAME_DATA_PATH = r\"D:\\University of Illinois Chicago\\Classes\\Hackathon\\esportsManagerChallenge\\data\\vct-international\\games\\2024\"\n",
    "LEAGUES = \"leagues.json\"\n",
    "MAPPING_DATA_V2 = \"mapping_data_v2.json\"\n",
    "# MAPPING_DATA = \"mapping_data.json\"\n",
    "PLAYERS = \"players.json\"\n",
    "TEAMS = \"teams.json\"\n",
    "TOURNAMENTS = \"tournaments.json\"\n",
    "ESPORTS_DATA = [\n",
    "    LEAGUES,\n",
    "    MAPPING_DATA_V2,\n",
    "    # MAPPING_DATA,\n",
    "    PLAYERS,\n",
    "    TEAMS,\n",
    "    TOURNAMENTS\n",
    "]\n",
    "RAW_DATA_PATHS = {\n",
    "    'Game-Changers': GAME_CHANGERS_GAME_DATA_PATH,\n",
    "    'VCT-Challengers': VCT_CHALLENGERS_GAME_DATA_PATH,\n",
    "    'VCT-International': VCT_INTERNATIONAL_GAME_DATA_PATH\n",
    "}\n",
    "PROCESSED_DATA_PATHS = {\n",
    "    'Game-Changers': r'D:\\University of Illinois Chicago\\Classes\\Hackathon\\esportsManagerChallenge\\processd-data\\game-changers',\n",
    "    'VCT-Challengers': r'D:\\University of Illinois Chicago\\Classes\\Hackathon\\esportsManagerChallenge\\processd-data\\vct-challengers',\n",
    "    'VCT-International': r'D:\\University of Illinois Chicago\\Classes\\Hackathon\\esportsManagerChallenge\\processd-data\\vct-international'\n",
    "}\n",
    "# a list of game events\n",
    "SPIKE_PLANT_STOPPED = 'spikePlantStopped'\n",
    "SPIKE_STATUS = 'spikeStatus'\n",
    "PLAYER_SPAWN = 'playerSpawn'\n",
    "SPIKE_DEFUSE_STARTED = 'spikeDefuseStarted'\n",
    "GAME_PHASE = 'gamePhase'\n",
    "METADATA = 'metadata'\n",
    "ROUND_CEREMONY = 'roundCeremony'\n",
    "SNAPSHOT = 'snapshot'\n",
    "SPIKE_PLANT_STARTED = 'spikePlantStarted'\n",
    "ROUND_DECIDED = 'roundDecided'\n",
    "SPIKE_DEFUSE_CHECKPOINT = 'spikeDefuseCheckpointReached'\n",
    "PLATFORM_GAME_ID = 'platformGameId'\n",
    "SPIKE_DEFUSED_STOPPED = 'spikeDefuseStopped'\n",
    "ABILITY_USED = 'abilityUsed'\n",
    "ROUND_ENDED = 'roundEnded'\n",
    "DAMAGE_EVENT = 'damageEvent'\n",
    "INVENTORY_TRANSACTION = 'inventoryTransaction'\n",
    "OBSERVER_TARGET = 'observerTarget'\n",
    "PLAYER_REVIVED = 'playerRevived'\n",
    "CONFIG = 'configuration'\n",
    "ROUND_STARTED = 'roundStarted'\n",
    "GAME_DECIDED = 'gameDecided'\n",
    "SPIKE_PLANT_COMPLETED = 'spikePlantCompleted'\n",
    "PLAYER_DIED = 'playerDied'\n",
    "\n",
    "GAME_EVENTS = [\n",
    "    SPIKE_PLANT_STOPPED,\n",
    "    SPIKE_STATUS,\n",
    "    PLAYER_SPAWN,\n",
    "    SPIKE_DEFUSE_STARTED,\n",
    "    GAME_PHASE,\n",
    "    METADATA,\n",
    "    ROUND_CEREMONY,\n",
    "    SNAPSHOT,\n",
    "    SPIKE_PLANT_STARTED,\n",
    "    ROUND_DECIDED,\n",
    "    SPIKE_DEFUSE_CHECKPOINT,\n",
    "    PLATFORM_GAME_ID,\n",
    "    SPIKE_DEFUSED_STOPPED,\n",
    "    ABILITY_USED,\n",
    "    ROUND_ENDED,\n",
    "    DAMAGE_EVENT,\n",
    "    INVENTORY_TRANSACTION,\n",
    "    OBSERVER_TARGET,\n",
    "    PLAYER_REVIVED,\n",
    "    CONFIG,\n",
    "    ROUND_STARTED,\n",
    "    GAME_DECIDED,\n",
    "    SPIKE_PLANT_COMPLETED,\n",
    "    PLAYER_DIED\n",
    "]\n",
    "\n",
    "AGENT_TYPES_FILE = r'D:\\University of Illinois Chicago\\Classes\\Hackathon\\esportsManagerChallenge\\agent_types.json'\n",
    "AGENT_MAPPING_FILE = r'D:\\University of Illinois Chicago\\Classes\\Hackathon\\esportsManagerChallenge\\agent_mapping.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_to_dataframe(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "VCTInternationalDataframes = {}\n",
    "VCTChallengersDataframes = {}\n",
    "VCTGameChangersDataframes = {}\n",
    "\n",
    "for json_file in ESPORTS_DATA:\n",
    "    file_path = os.path.join(VCT_INTERNATIONAL_ESPORTS_PATH, json_file)\n",
    "    VCTInternationalDataframes[json_file] = load_json_to_dataframe(file_path)\n",
    "\n",
    "    file_path = os.path.join(VCT_CHALLENGERS_ESPORTS_PATH, json_file)\n",
    "    VCTChallengersDataframes[json_file] = load_json_to_dataframe(file_path)\n",
    "\n",
    "    file_path = os.path.join(GAME_CHANGERS_ESPORTS_PATH, json_file)\n",
    "    VCTGameChangersDataframes[json_file] = load_json_to_dataframe(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the dataframes to have consistent naming conventions and remove unnecessary columns\n",
    "def processDataFrame(dframe):\n",
    "    for name, df in dframe.items():\n",
    "        if name == LEAGUES:\n",
    "            # Drop \"dark_logo_url\" and \"light_logo_url\" \"slug\" columns\n",
    "            df.drop(columns=['dark_logo_url', 'light_logo_url', 'slug'], inplace=True)\n",
    "\n",
    "            # Rename \"name\" and \"slug\" to \"league_name\"\n",
    "            df.rename(columns={'name': 'league_name'}, inplace=True)\n",
    "\n",
    "        elif name == MAPPING_DATA_V2:\n",
    "            # convert from camel case to snake_case\n",
    "            df.columns = (\n",
    "                df.columns\n",
    "                .str.replace('(.)([A-Z][a-z]+)', r'\\1_\\2', regex=True)\n",
    "                .str.replace('([a-z0-9])([A-Z])', r'\\1_\\2', regex=True)\n",
    "                .str.lower()\n",
    "            )\n",
    "        \n",
    "        # elif name == MAPPING_DATA:\n",
    "        #     # convert from camel case to snake_case\n",
    "        #     df.columns = (\n",
    "        #         df.columns\n",
    "        #         .str.replace('(.)([A-Z][a-z]+)', r'\\1_\\2', regex=True)\n",
    "        #         .str.replace('([a-z0-9])([A-Z])', r'\\1_\\2', regex=True)\n",
    "        #         .str.lower()\n",
    "        #     )\n",
    "            \n",
    "        #     print(f\"Processed {name}:\")\n",
    "        #     print(df.head())\n",
    "\n",
    "        elif name == PLAYERS:\n",
    "            # update the column \"id\" to \"player_id\" and \"home_team_id\" to \"team_id\"\n",
    "            df.rename(columns={'id': 'player_id', 'home_team_id': 'team_id'}, inplace=True)\n",
    "\n",
    "            # drop the column \"photo_url\"\n",
    "            df.drop(columns=['photo_url'], inplace=True)\n",
    "\n",
    "        elif name == TEAMS:\n",
    "            # update id to team_id, name to team_name, slug to team_slug, home_league_id to league_id\n",
    "            df.rename(columns={'id': 'team_id', 'name': 'team_name', 'home_league_id': 'league_id'}, inplace=True)\n",
    "\n",
    "            # drop columns dark_logo_url and light_logo_url and acronym, slug\n",
    "            df.drop(columns=['dark_logo_url', 'light_logo_url', 'acronym', 'slug'], inplace=True)\n",
    "\n",
    "        elif name == TOURNAMENTS:\n",
    "            # update id to tournament_id\n",
    "            df.rename(columns={'id': 'tournament_id', 'name': 'tournament_name'}, inplace=True)\n",
    "\n",
    "            # drop column \"time_zone\", \"status\"\n",
    "            df.drop(columns=['time_zone', 'status'], inplace=True)\n",
    "\n",
    "    return dframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "VCTInternationalDataframes = processDataFrame(VCTInternationalDataframes)\n",
    "VCTChallengersDataframes = processDataFrame(VCTChallengersDataframes)\n",
    "VCTGameChangersDataframes = processDataFrame(VCTGameChangersDataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to process the players.json file in the different esports datasets\n",
    "def process_player_data(players_df):\n",
    "    # convert time to datetime\n",
    "    players_df['created_at'] = pd.to_datetime(players_df['created_at'])\n",
    "    players_df['updated_at'] = pd.to_datetime(players_df['updated_at'])\n",
    "\n",
    "    # Define aggregation functions\n",
    "    def consolidate_player_entries(group):\n",
    "        # Sort the group by 'updated_at' in ascending order\n",
    "        group_sorted = group.sort_values('updated_at')\n",
    "\n",
    "        consolidated_entry = {}\n",
    "        consolidated_entry['player_id'] = group['player_id'].iloc[0]\n",
    "        consolidated_entry['handle'] = group['handle'].iloc[0]\n",
    "        consolidated_entry['first_name'] = group['first_name'].iloc[0]\n",
    "        consolidated_entry['last_name'] = group['last_name'].iloc[0]\n",
    "        consolidated_entry['status'] = group_sorted['status'].iloc[-1]\n",
    "\n",
    "        # Get the 'team_id' from the latest 'updated_at'\n",
    "        consolidated_entry['team_id'] = group_sorted['team_id'].iloc[-1]\n",
    "\n",
    "        consolidated_entry['earliest_created_at'] = group['created_at'].min()\n",
    "        consolidated_entry['latest_updated_at'] = group['updated_at'].max()\n",
    "\n",
    "        return pd.Series(consolidated_entry)\n",
    "    \n",
    "    # Consolidate player entries without including grouping columns\n",
    "    consolidated_players_df = players_df.groupby('player_id', group_keys=False).apply(consolidate_player_entries).reset_index(drop=True)\n",
    "\n",
    "    # Function to create team history for each player\n",
    "    def create_team_history(group):\n",
    "        team_history = group.groupby('team_id', as_index=False).agg({\n",
    "            'created_at': 'min',\n",
    "            'updated_at': 'max'\n",
    "        })\n",
    "        team_history['player_id'] = group['player_id'].iloc[0]\n",
    "        return team_history\n",
    "\n",
    "    # Apply the function to each player group\n",
    "    team_history_df = players_df.groupby('player_id', group_keys=False).apply(create_team_history).reset_index(drop=True)\n",
    "\n",
    "    # Convert team history to a dictionary per player\n",
    "    team_history_dict = team_history_df.groupby('player_id')[['team_id', 'created_at', 'updated_at']].apply(lambda x: x.to_dict('records')).to_dict()\n",
    "\n",
    "\n",
    "    # Add team history to the consolidated players DataFrame\n",
    "    consolidated_players_df['team_history'] = consolidated_players_df['player_id'].map(team_history_dict)\n",
    "\n",
    "    return consolidated_players_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manan\\AppData\\Local\\Temp\\ipykernel_27108\\2308614703.py:28: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  consolidated_players_df = players_df.groupby('player_id', group_keys=False).apply(consolidate_player_entries).reset_index(drop=True)\n",
      "C:\\Users\\Manan\\AppData\\Local\\Temp\\ipykernel_27108\\2308614703.py:40: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  team_history_df = players_df.groupby('player_id', group_keys=False).apply(create_team_history).reset_index(drop=True)\n",
      "C:\\Users\\Manan\\AppData\\Local\\Temp\\ipykernel_27108\\2308614703.py:28: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  consolidated_players_df = players_df.groupby('player_id', group_keys=False).apply(consolidate_player_entries).reset_index(drop=True)\n",
      "C:\\Users\\Manan\\AppData\\Local\\Temp\\ipykernel_27108\\2308614703.py:40: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  team_history_df = players_df.groupby('player_id', group_keys=False).apply(create_team_history).reset_index(drop=True)\n",
      "C:\\Users\\Manan\\AppData\\Local\\Temp\\ipykernel_27108\\2308614703.py:28: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  consolidated_players_df = players_df.groupby('player_id', group_keys=False).apply(consolidate_player_entries).reset_index(drop=True)\n",
      "C:\\Users\\Manan\\AppData\\Local\\Temp\\ipykernel_27108\\2308614703.py:40: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  team_history_df = players_df.groupby('player_id', group_keys=False).apply(create_team_history).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# process the player data\n",
    "VCTInternationalDataframes[PLAYERS] = process_player_data(VCTInternationalDataframes[PLAYERS])\n",
    "VCTChallengersDataframes[PLAYERS] = process_player_data(VCTChallengersDataframes[PLAYERS])\n",
    "VCTGameChangersDataframes[PLAYERS] = process_player_data(VCTGameChangersDataframes[PLAYERS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_leagues_and_tournaments(leagues_df, tournaments_df):\n",
    "    # Group the tournaments by 'league_id' and aggregate 'tournament_id' and 'tournament_name' as a list of dictionaries\n",
    "    tournaments_grouped = tournaments_df.groupby('league_id').apply(\n",
    "        lambda x: x[['tournament_id', 'tournament_name']].to_dict('records')\n",
    "    ).reset_index(name='tournaments')\n",
    "\n",
    "    # Merge the tournaments into the leagues DataFrame\n",
    "    leagues_with_tournaments = pd.merge(\n",
    "        leagues_df,\n",
    "        tournaments_grouped,\n",
    "        on='league_id',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    return leagues_with_tournaments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manan\\AppData\\Local\\Temp\\ipykernel_27108\\3123475813.py:3: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  tournaments_grouped = tournaments_df.groupby('league_id').apply(\n",
      "C:\\Users\\Manan\\AppData\\Local\\Temp\\ipykernel_27108\\3123475813.py:3: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  tournaments_grouped = tournaments_df.groupby('league_id').apply(\n",
      "C:\\Users\\Manan\\AppData\\Local\\Temp\\ipykernel_27108\\3123475813.py:3: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  tournaments_grouped = tournaments_df.groupby('league_id').apply(\n"
     ]
    }
   ],
   "source": [
    "# Process leagues and tournaments for each version\n",
    "VCTInternationalDataframes[LEAGUES] = process_leagues_and_tournaments(\n",
    "    VCTInternationalDataframes[LEAGUES], VCTInternationalDataframes[TOURNAMENTS]\n",
    ")\n",
    "VCTChallengersDataframes[LEAGUES] = process_leagues_and_tournaments(\n",
    "    VCTChallengersDataframes[LEAGUES], VCTChallengersDataframes[TOURNAMENTS]\n",
    ")\n",
    "VCTGameChangersDataframes[LEAGUES] = process_leagues_and_tournaments(\n",
    "    VCTGameChangersDataframes[LEAGUES], VCTGameChangersDataframes[TOURNAMENTS]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_teams_dataframe(teams_df):\n",
    "    # Group by 'team_id' and 'team_name', aggregate 'league_id' into a list of unique league_ids\n",
    "    processed_teams_df = teams_df.groupby(['team_id', 'team_name']).agg({\n",
    "        'league_id': lambda x: list(set(x))\n",
    "    }).reset_index()\n",
    "    return processed_teams_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "VCTInternationalDataframes[TEAMS] = process_teams_dataframe(VCTInternationalDataframes[TEAMS])\n",
    "VCTChallengersDataframes[TEAMS] = process_teams_dataframe(VCTChallengersDataframes[TEAMS])\n",
    "VCTGameChangersDataframes[TEAMS] = process_teams_dataframe(VCTGameChangersDataframes[TEAMS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved leagues.json to D:\\University of Illinois Chicago\\Classes\\Hackathon\\esportsManagerChallenge\\processd-data\\vct-international\\leagues.json.json\n",
      "Saved mapping_data_v2.json to D:\\University of Illinois Chicago\\Classes\\Hackathon\\esportsManagerChallenge\\processd-data\\vct-international\\mapping_data_v2.json.json\n",
      "Saved players.json to D:\\University of Illinois Chicago\\Classes\\Hackathon\\esportsManagerChallenge\\processd-data\\vct-international\\players.json.json\n",
      "Saved teams.json to D:\\University of Illinois Chicago\\Classes\\Hackathon\\esportsManagerChallenge\\processd-data\\vct-international\\teams.json.json\n",
      "Saved tournaments.json to D:\\University of Illinois Chicago\\Classes\\Hackathon\\esportsManagerChallenge\\processd-data\\vct-international\\tournaments.json.json\n",
      "Saved leagues.json to D:\\University of Illinois Chicago\\Classes\\Hackathon\\esportsManagerChallenge\\processd-data\\vct-challengers\\leagues.json.json\n",
      "Saved mapping_data_v2.json to D:\\University of Illinois Chicago\\Classes\\Hackathon\\esportsManagerChallenge\\processd-data\\vct-challengers\\mapping_data_v2.json.json\n",
      "Saved players.json to D:\\University of Illinois Chicago\\Classes\\Hackathon\\esportsManagerChallenge\\processd-data\\vct-challengers\\players.json.json\n",
      "Saved teams.json to D:\\University of Illinois Chicago\\Classes\\Hackathon\\esportsManagerChallenge\\processd-data\\vct-challengers\\teams.json.json\n",
      "Saved tournaments.json to D:\\University of Illinois Chicago\\Classes\\Hackathon\\esportsManagerChallenge\\processd-data\\vct-challengers\\tournaments.json.json\n",
      "Saved leagues.json to D:\\University of Illinois Chicago\\Classes\\Hackathon\\esportsManagerChallenge\\processd-data\\game-changers\\leagues.json.json\n",
      "Saved mapping_data_v2.json to D:\\University of Illinois Chicago\\Classes\\Hackathon\\esportsManagerChallenge\\processd-data\\game-changers\\mapping_data_v2.json.json\n",
      "Saved players.json to D:\\University of Illinois Chicago\\Classes\\Hackathon\\esportsManagerChallenge\\processd-data\\game-changers\\players.json.json\n",
      "Saved teams.json to D:\\University of Illinois Chicago\\Classes\\Hackathon\\esportsManagerChallenge\\processd-data\\game-changers\\teams.json.json\n",
      "Saved tournaments.json to D:\\University of Illinois Chicago\\Classes\\Hackathon\\esportsManagerChallenge\\processd-data\\game-changers\\tournaments.json.json\n"
     ]
    }
   ],
   "source": [
    "# Save the processed dataframes to json files in the processed-data folder for each version\n",
    "\n",
    "def save_dataframes_to_json(dataframes, version):\n",
    "    for name, df in dataframes.items():\n",
    "        file_path = os.path.join(PROCESSED_DATA_PATHS[version], f\"{name}.json\")\n",
    "        # check the file path, if it does not exist, create it\n",
    "        if not os.path.exists(os.path.dirname(file_path)):\n",
    "            os.makedirs(os.path.dirname(file_path))\n",
    "        df.to_json(file_path, orient='records')\n",
    "        print(f\"Saved {name} to {file_path}\")\n",
    "    \n",
    "save_dataframes_to_json(VCTInternationalDataframes, 'VCT-International')\n",
    "save_dataframes_to_json(VCTChallengersDataframes, 'VCT-Challengers')\n",
    "save_dataframes_to_json(VCTGameChangersDataframes, 'Game-Changers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconcile_mapping_with_files(mapping_df, games_directory):\n",
    "    # Step 1: List all JSON files in the directory\n",
    "    json_files = [f for f in os.listdir(games_directory) if f.endswith('.json')]\n",
    "    \n",
    "    # Step 2: Extract IDs from filenames\n",
    "    file_ids = set()\n",
    "    for filename in json_files:\n",
    "        # Remove 'val_' prefix and '.json' suffix, and replace '_' with ':'\n",
    "        file_id = filename.replace('val_', 'val:').replace('.json', '')\n",
    "        file_ids.add(file_id)\n",
    "    \n",
    "    # Step 3: Extract 'platform_game_id's from mapping_data_v2 DataFrame\n",
    "    mapping_ids = set(mapping_df['platform_game_id'])\n",
    "    \n",
    "    # Step 4: Find the intersection of IDs present in both\n",
    "    ids_in_both = file_ids & mapping_ids\n",
    "    \n",
    "    # Step 5: Filter mapping_df to only include entries present in the JSON files\n",
    "    mapping_df_filtered = mapping_df[mapping_df['platform_game_id'].isin(ids_in_both)].copy()\n",
    "    \n",
    "    # Optional: Report counts\n",
    "    print(f\"Total JSON files in directory: {len(file_ids)}\")\n",
    "    print(f\"Total platform_game_ids in mapping_data_v2: {len(mapping_ids)}\")\n",
    "    print(f\"Number of matching IDs: {len(ids_in_both)}\")\n",
    "    print(f\"Entries dropped from mapping_data_v2: {len(mapping_ids) - len(ids_in_both)}\")\n",
    "    \n",
    "    return mapping_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total JSON files in directory: 959\n",
      "Total platform_game_ids in mapping_data_v2: 1742\n",
      "Number of matching IDs: 959\n",
      "Entries dropped from mapping_data_v2: 783\n",
      "Total JSON files in directory: 2314\n",
      "Total platform_game_ids in mapping_data_v2: 4475\n",
      "Number of matching IDs: 2314\n",
      "Entries dropped from mapping_data_v2: 2161\n",
      "Total JSON files in directory: 422\n",
      "Total platform_game_ids in mapping_data_v2: 1140\n",
      "Number of matching IDs: 422\n",
      "Entries dropped from mapping_data_v2: 718\n"
     ]
    }
   ],
   "source": [
    "# filter the mapping data for each version\n",
    "VCTInternationalDataframes[MAPPING_DATA_V2] = reconcile_mapping_with_files(\n",
    "    VCTInternationalDataframes[MAPPING_DATA_V2], VCT_INTERNATIONAL_GAME_DATA_PATH\n",
    ")\n",
    "VCTChallengersDataframes[MAPPING_DATA_V2] = reconcile_mapping_with_files(\n",
    "    VCTChallengersDataframes[MAPPING_DATA_V2], VCT_CHALLENGERS_GAME_DATA_PATH\n",
    ")\n",
    "VCTGameChangersDataframes[MAPPING_DATA_V2] = reconcile_mapping_with_files(\n",
    "    VCTGameChangersDataframes[MAPPING_DATA_V2], GAME_CHANGERS_GAME_DATA_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a game data file\n",
    "def load_game_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_game_data(data):\n",
    "    # Initialize an empty dictionary to store the processed data\n",
    "    processed_data = {\n",
    "        'winner': None,      # Initialize winner at the top level\n",
    "        'total_rounds': 0    # Initialize total rounds counter\n",
    "    }\n",
    "    new_config = True\n",
    "    current_round = 1\n",
    "    max_round = 0  # To keep track of the highest round number encountered\n",
    "    event_queue = []\n",
    "    latest_defuser_id = None  # Variable to store the player who started defusing\n",
    "    latest_killer_id = None   # Variable to store the last killer in a kill event\n",
    "    first_blood_occurred = False  # Variable to track if first blood has occurred in the current round\n",
    "\n",
    "    # Function to update configuration\n",
    "    def update_configuration(config_data, current_round):\n",
    "        # Update the map name if it is not already present in the processed data\n",
    "        if 'map' not in processed_data:\n",
    "            if 'selectedMap' in config_data and 'fallback' in config_data['selectedMap'] and 'displayName' in config_data['selectedMap']['fallback']:\n",
    "                map_name = config_data['selectedMap']['fallback']['displayName']\n",
    "            else:\n",
    "                map_name = 'UNKNOWN'\n",
    "            processed_data['map'] = map_name\n",
    "\n",
    "        # Initialize team1 and team2 in processed_data if not already present\n",
    "        if 'team1' not in processed_data or 'team2' not in processed_data:\n",
    "            # Initialize team1 and team2 in processed_data\n",
    "            teams = config_data.get('teams', [])\n",
    "            if len(teams) != 2:\n",
    "                print(\"Warning: Expected 2 teams, found {}\".format(len(teams)))\n",
    "                return\n",
    "            processed_data['team1'] = {'won_rounds': 0, 'aces': 0, 'players': {}}\n",
    "            processed_data['team2'] = {'won_rounds': 0, 'aces': 0, 'players': {}}\n",
    "\n",
    "            # Build a mapping from player IDs to their info\n",
    "            player_info_map = {}\n",
    "            for player in config_data.get('players', []):\n",
    "                player_id = player['playerId']['value']\n",
    "                selected_agent_guid = player['selectedAgent']['fallback'].get('guid', 'UNKNOWN')\n",
    "                player_info_map[player_id] = {\n",
    "                    'player_id': player_id,\n",
    "                    'selectedAgent': selected_agent_guid\n",
    "                }\n",
    "\n",
    "            # Assign players to teams\n",
    "            team_keys = ['team1', 'team2']\n",
    "            for team_key, team in zip(team_keys, teams):\n",
    "                team_id = team['teamId']['value']\n",
    "                processed_data[team_key]['team_id'] = team_id\n",
    "\n",
    "                for player in team.get('playersInTeam', []):\n",
    "                    player_id = player['value']\n",
    "                    player_info = player_info_map.get(player_id, {})\n",
    "                    round_key = f'round{current_round}'\n",
    "                    processed_data[team_key]['players'][player_id] = {\n",
    "                        'player_id': player_id,\n",
    "                        'kills': 0,\n",
    "                        'deaths': 0,\n",
    "                        'defuses': 0,        # Initialize defuses counter\n",
    "                        'revivals': 0,       # Initialize revivals counter\n",
    "                        'plants': 0,         # Initialize plants counter\n",
    "                        'clutches': 0,       # Initialize clutches counter\n",
    "                        'first_bloods': 0,   # Initialize first bloods counter\n",
    "                        'multiKillCount': 0, # Initialize multi-kill counter\n",
    "                        round_key: {\n",
    "                            'selectedAgent': player_info.get('selectedAgent', 'UNKNOWN'),\n",
    "                            'damage_events': [],\n",
    "                            'damage_taken': 0,\n",
    "                            'kills_in_round': 0,  # Initialize kills in round\n",
    "                            'deaths_in_round': 0  # NEW\n",
    "                        }\n",
    "                    }\n",
    "        else:\n",
    "            # Teams already initialized, only update players' selectedAgent for the current round\n",
    "            # Build a mapping from player IDs to their selected agents\n",
    "            player_info_map = {}\n",
    "            for player in config_data.get('players', []):\n",
    "                player_id = player['playerId']['value']\n",
    "                selected_agent_guid = player['selectedAgent']['fallback'].get('guid', 'UNKNOWN')\n",
    "                player_info_map[player_id] = selected_agent_guid\n",
    "\n",
    "            # Assign selectedAgent to players for the current round\n",
    "            team_keys = ['team1', 'team2']\n",
    "            for team_key in team_keys:\n",
    "                for player_id in processed_data[team_key]['players']:\n",
    "                    round_key = f'round{current_round}'\n",
    "                    selected_agent = player_info_map.get(player_id, 'UNKNOWN')\n",
    "                    # Update the player's data for the current round\n",
    "                    processed_data[team_key]['players'][player_id][round_key] = {\n",
    "                        'selectedAgent': selected_agent,\n",
    "                        'damage_events': [],\n",
    "                        'damage_taken': 0,\n",
    "                        'kills_in_round': 0  # Initialize kills in round\n",
    "                    }\n",
    "\n",
    "    # Function to process queued events\n",
    "    def process_queued_events():\n",
    "        nonlocal event_queue\n",
    "        for event in event_queue:\n",
    "            handle_event(event)\n",
    "        event_queue = []  # Clear the queue after processing\n",
    "\n",
    "    # Function to handle individual events\n",
    "    def handle_event(event):\n",
    "        nonlocal new_config, current_round, latest_defuser_id, latest_killer_id, first_blood_occurred, max_round\n",
    "        for event_name in event.keys():\n",
    "            if event_name == PLATFORM_GAME_ID and PLATFORM_GAME_ID not in processed_data:\n",
    "                processed_data[PLATFORM_GAME_ID] = event[PLATFORM_GAME_ID]\n",
    "\n",
    "            elif event_name == CONFIG:\n",
    "                if new_config:\n",
    "                    # Update the current round number from the configuration if available\n",
    "                    config_data = event[CONFIG]\n",
    "                    current_round = config_data.get('spikeMode', {}).get('currentRound', current_round)\n",
    "                    # Update configuration data\n",
    "                    update_configuration(config_data, current_round)\n",
    "                    new_config = False\n",
    "                    # Process any queued events after configuration is set\n",
    "                    process_queued_events()\n",
    "\n",
    "            elif event_name == ROUND_ENDED or event_name == ROUND_DECIDED:\n",
    "                # When a round ends, set new_config to True for the next round\n",
    "                new_config = True\n",
    "                # Handle ROUND_DECIDED event to update won_rounds\n",
    "                if event_name == ROUND_DECIDED:\n",
    "                    result = event[ROUND_DECIDED]['result']\n",
    "                    winning_team_info = result.get('winningTeam', {})\n",
    "                    winning_team_id = winning_team_info.get('value', None)\n",
    "                    cause = result.get('spikeModeResult', {}).get('cause', '')\n",
    "                    # Increment won_rounds for the winning team if there is a winner\n",
    "                    if winning_team_id is not None:\n",
    "                        for team_key in ['team1', 'team2']:\n",
    "                            if processed_data[team_key]['team_id'] == winning_team_id:\n",
    "                                processed_data[team_key]['won_rounds'] += 1\n",
    "                                break\n",
    "                    # If the cause is 'SPIKE_DEFUSE', increment defuses for the player\n",
    "                    if cause == 'SPIKE_DEFUSE' and latest_defuser_id is not None:\n",
    "                        # Find the player and increment defuses\n",
    "                        for team_key in ['team1', 'team2']:\n",
    "                            if latest_defuser_id in processed_data[team_key]['players']:\n",
    "                                player_data = processed_data[team_key]['players'][latest_defuser_id]\n",
    "                                player_data['defuses'] += 1\n",
    "                                break\n",
    "                # After processing the round, check for multikills\n",
    "                for team_key in ['team1', 'team2']:\n",
    "                    for player_id, player_data in processed_data[team_key]['players'].items():\n",
    "                        round_key = f'round{current_round}'\n",
    "                        if round_key in player_data:\n",
    "                            kills_in_round = player_data[round_key].get('kills_in_round', 0)\n",
    "                            if kills_in_round > 1:\n",
    "                                player_data['multiKillCount'] += 1\n",
    "\n",
    "            elif event_name == ROUND_STARTED:\n",
    "                # A new round has started\n",
    "                new_config = True\n",
    "                # Update the current round number\n",
    "                current_round = event[ROUND_STARTED].get('round', current_round + 1)\n",
    "                max_round = max(max_round, current_round)  # Update max_round\n",
    "                # Reset latest_defuser_id and latest_killer_id at the start of a new round\n",
    "                latest_defuser_id = None\n",
    "                latest_killer_id = None\n",
    "                first_blood_occurred = False  # Reset first blood flag\n",
    "\n",
    "            elif event_name == SPIKE_DEFUSE_STARTED:\n",
    "                # Update the latest_defuser_id\n",
    "                latest_defuser_id = event[SPIKE_DEFUSE_STARTED]['playerId']['value']\n",
    "\n",
    "            elif event_name == PLAYER_REVIVED:\n",
    "                # Handle PLAYER_REVIVED event\n",
    "                revived_by_id = event[PLAYER_REVIVED]['revivedById']['value']\n",
    "                # Increment revivals counter for the player who did the revival\n",
    "                for team_key in ['team1', 'team2']:\n",
    "                    if revived_by_id in processed_data[team_key]['players']:\n",
    "                        player_data = processed_data[team_key]['players'][revived_by_id]\n",
    "                        player_data['revivals'] += 1\n",
    "                        break\n",
    "\n",
    "            elif event_name == GAME_DECIDED:\n",
    "                # Handle GAME_DECIDED event\n",
    "                state = event[GAME_DECIDED].get('state', '')\n",
    "                if state == 'WINNER_DECIDED':\n",
    "                    winning_team_id = event[GAME_DECIDED]['winningTeam']['value']\n",
    "                    # Set the winner at the top level\n",
    "                    processed_data['winner'] = winning_team_id\n",
    "                elif state == 'DRAW':\n",
    "                    # It's a draw; 'winner' remains None\n",
    "                    processed_data['winner'] = None\n",
    "\n",
    "            elif event_name == SPIKE_PLANT_COMPLETED:\n",
    "                # Handle SPIKE_PLANT_COMPLETED event\n",
    "                planter_id = event[SPIKE_PLANT_COMPLETED]['playerId']['value']\n",
    "                # Increment plants counter for the player who planted the spike\n",
    "                for team_key in ['team1', 'team2']:\n",
    "                    if planter_id in processed_data[team_key]['players']:\n",
    "                        player_data = processed_data[team_key]['players'][planter_id]\n",
    "                        player_data['plants'] += 1\n",
    "                        break\n",
    "\n",
    "            elif event_name == ROUND_CEREMONY:\n",
    "                # Handle ROUND_CEREMONY event\n",
    "                ceremony_type = event[ROUND_CEREMONY].get('type', 'DEFAULT')\n",
    "                if ceremony_type == 'CLUTCH':\n",
    "                    # Increment 'clutches' counter for the latest killer\n",
    "                    if latest_killer_id is not None:\n",
    "                        # Find the player and increment clutches\n",
    "                        for team_key in ['team1', 'team2']:\n",
    "                            if latest_killer_id in processed_data[team_key]['players']:\n",
    "                                player_data = processed_data[team_key]['players'][latest_killer_id]\n",
    "                                player_data['clutches'] += 1\n",
    "                                break\n",
    "                elif ceremony_type == 'ACE':\n",
    "                    # Increment 'aces' counter for the team of the latest killer\n",
    "                    if latest_killer_id is not None:\n",
    "                        for team_key in ['team1', 'team2']:\n",
    "                            if latest_killer_id in processed_data[team_key]['players']:\n",
    "                                processed_data[team_key]['aces'] += 1\n",
    "                                break\n",
    "                # No action needed for 'DEFAULT' and 'THRIFTY'\n",
    "\n",
    "            elif event_name == DAMAGE_EVENT:\n",
    "                # Handle DAMAGE_EVENT\n",
    "                damage_event = event[DAMAGE_EVENT]\n",
    "                causer_id = damage_event.get('causerId', {}).get('value')\n",
    "                victim_id = damage_event.get('victimId', {}).get('value')\n",
    "                kill_event = damage_event.get('killEvent', False)\n",
    "                location = damage_event.get('location', 'UNKNOWN')\n",
    "                damage_amount = damage_event.get('damageDealt', damage_event.get('damageAmount', 0))\n",
    "                round_key = f'round{current_round}'\n",
    "\n",
    "                # Standardize the location: only keep \"HEAD\" and default others to \"BODY\"\n",
    "                if location != \"HEAD\":\n",
    "                    location = \"BODY\"\n",
    "\n",
    "                # Determine weapon or ability used\n",
    "                weapon_info = damage_event.get('weapon')\n",
    "                ability_info = damage_event.get('ability')\n",
    "\n",
    "                if weapon_info:\n",
    "                    # Damage was caused by a weapon\n",
    "                    weapon_guid = weapon_info.get('fallback', {}).get('guid', 'UNKNOWN')\n",
    "                    damage_source = {\n",
    "                        'type': 'weapon',\n",
    "                        'guid': weapon_guid\n",
    "                    }\n",
    "                elif ability_info:\n",
    "                    # Damage was caused by an ability\n",
    "                    ability_slot = ability_info.get('fallback', {}).get('inventorySlot', {}).get('slot', 'UNKNOWN')\n",
    "                    damage_source = {\n",
    "                        'type': 'ability',\n",
    "                        'slot': ability_slot\n",
    "                    }\n",
    "                else:\n",
    "                    # Unknown damage source\n",
    "                    damage_source = {\n",
    "                        'type': 'unknown',\n",
    "                        'info': {}\n",
    "                    }\n",
    "\n",
    "                # Update processed_data for the causer (attacker)\n",
    "                for team_key in ['team1', 'team2']:\n",
    "                    if causer_id in processed_data[team_key]['players']:\n",
    "                        player_data = processed_data[team_key]['players'][causer_id]\n",
    "                        # Initialize round data if not present\n",
    "                        if round_key not in player_data:\n",
    "                            player_data[round_key] = {\n",
    "                                'selectedAgent': 'UNKNOWN',\n",
    "                                'damage_events': [],\n",
    "                                'damage_taken': 0,\n",
    "                                'kills_in_round': 0  # Initialize kills in round\n",
    "                            }\n",
    "                        # Ensure damage_events is initialized\n",
    "                        if 'damage_events' not in player_data[round_key]:\n",
    "                            player_data[round_key]['damage_events'] = []\n",
    "                        # Append damage event (exclude victim_id)\n",
    "                        player_data[round_key]['damage_events'].append({\n",
    "                            'damage_amount': damage_amount,\n",
    "                            'location': location,\n",
    "                            'damage_source': damage_source\n",
    "                        })\n",
    "                        # Increment kill count if killEvent is True\n",
    "                        if kill_event:\n",
    "                            player_data['kills'] = player_data.get('kills', 0) + 1\n",
    "                            # Update latest_killer_id\n",
    "                            latest_killer_id = causer_id\n",
    "                            # Check for first blood\n",
    "                            if not first_blood_occurred:\n",
    "                                player_data['first_bloods'] += 1\n",
    "                                first_blood_occurred = True\n",
    "                            # Increment kills_in_round\n",
    "                            player_data[round_key]['kills_in_round'] = player_data[round_key].get('kills_in_round', 0) + 1\n",
    "                        break\n",
    "\n",
    "                # Update processed_data for the victim\n",
    "                for team_key in ['team1', 'team2']:\n",
    "                    if victim_id in processed_data[team_key]['players']:\n",
    "                        player_data = processed_data[team_key]['players'][victim_id]\n",
    "                        # Initialize round data if not present\n",
    "                        if round_key not in player_data:\n",
    "                            player_data[round_key] = {\n",
    "                                'selectedAgent': 'UNKNOWN',\n",
    "                                'damage_events': [],\n",
    "                                'damage_taken': 0,\n",
    "                                'kills_in_round': 0  # Initialize kills in round\n",
    "                            }\n",
    "                        # Add damage amount to total damage taken\n",
    "                        player_data[round_key]['damage_taken'] += damage_amount\n",
    "                        # Increment death count if killEvent is True\n",
    "                        if kill_event:\n",
    "                            player_data['deaths'] = player_data.get('deaths', 0) + 1\n",
    "                            player_data[round_key]['deaths_in_round'] = player_data[round_key].get('deaths_in_round', 0) + 1\n",
    "                        break\n",
    "\n",
    "    # Iterate over each event in the data\n",
    "    for idx, event in enumerate(data):\n",
    "        # If configuration is not set and event is not PLATFORM_GAME_ID or CONFIG, queue the event\n",
    "        if 'team1' not in processed_data and 'team2' not in processed_data:\n",
    "            if CONFIG in event or PLATFORM_GAME_ID in event:\n",
    "                handle_event(event)\n",
    "            else:\n",
    "                event_queue.append(event)\n",
    "        else:\n",
    "            handle_event(event)\n",
    "\n",
    "    # Set total_rounds in processed_data\n",
    "    processed_data['total_rounds'] = max_round\n",
    "\n",
    "    # Return the processed data\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process all games in a directory and collect processed_data\n",
    "def collect_processed_data_in_directory(directory_path):\n",
    "    all_processed_data = []  # List to hold processed data\n",
    "    invalid_games = []  # List to hold games with invalid conditions\n",
    "\n",
    "    # Iterate over all JSON files in the directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.json'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            data = load_game_data(file_path)\n",
    "            processed_data = process_game_data(data)\n",
    "\n",
    "            # Check for invalid conditions\n",
    "            invalid_reasons = []\n",
    "            if processed_data.get('winner') is None:\n",
    "                invalid_reasons.append('No valid winner')\n",
    "\n",
    "            total_rounds = processed_data.get('total_rounds', 0)\n",
    "            if total_rounds < 13:\n",
    "                invalid_reasons.append(f'Total rounds less than 13 ({total_rounds})')\n",
    "\n",
    "            if invalid_reasons:\n",
    "                invalid_games.append({\n",
    "                    'file': filename,\n",
    "                    'platform_game_id': processed_data.get('platformGameId', 'unknown'),\n",
    "                    'message': '; '.join(invalid_reasons)\n",
    "                })\n",
    "\n",
    "            all_processed_data.append(processed_data)\n",
    "\n",
    "    return all_processed_data, invalid_games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle processing and saving data for a single directory\n",
    "def process_single_directory(key, invalid_games_list):\n",
    "    input_directory = RAW_DATA_PATHS[key]\n",
    "    output_directory = PROCESSED_DATA_PATHS[key]\n",
    "\n",
    "    # Process games in the input directory\n",
    "    processed_games_data, invalid_games = collect_processed_data_in_directory(input_directory)\n",
    "    \n",
    "    # Append invalid games to the shared list\n",
    "    invalid_games_list.extend(invalid_games)\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # Write processed data to a JSON file in the output directory\n",
    "    output_file_path = os.path.join(output_directory, 'processed_game_data.json')\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        json.dump(processed_games_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Main function to process all game data directories in parallel\n",
    "def process_all_game_data_parallel():\n",
    "    invalid_games = []  # Shared list to collect all invalid games across directories\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        # Submit tasks for parallel processing\n",
    "        futures = [\n",
    "            executor.submit(process_single_directory, key, invalid_games)\n",
    "            for key in RAW_DATA_PATHS.keys()\n",
    "        ]\n",
    "\n",
    "        # Wait for all tasks to complete\n",
    "        for future in futures:\n",
    "            future.result()  # Raise any exceptions if they occurred\n",
    "\n",
    "    # Write all invalid games to a JSON file in the main folder\n",
    "    error_file_path = os.path.join(os.getcwd(), 'errors.json')\n",
    "    with open(error_file_path, 'w') as f:\n",
    "        json.dump(invalid_games, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_all_game_data_parallel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON data from a file\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Load processed game data for each version from the processed-data folder\n",
    "VCTInternationalProcessedGames = load_json(PROCESSED_DATA_PATHS['VCT-International'] + '/processed_game_data.json')\n",
    "VCTChallengersProcessedGames = load_json(PROCESSED_DATA_PATHS['VCT-Challengers'] + '/processed_game_data.json')\n",
    "VCTGameChangersProcessedGames = load_json(PROCESSED_DATA_PATHS['Game-Changers'] + '/processed_game_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'winner': None, 'total_rounds': 0, 'platformGameId': 'val:00676ae0-0a4d-4577-be1e-ab4d3a9889aa'}\n"
     ]
    }
   ],
   "source": [
    "print (VCTInternationalProcessedGames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              platform_game_id            match_id  \\\n",
      "1343  val:00676ae0-0a4d-4577-be1e-ab4d3a9889aa  112138727088677911   \n",
      "\n",
      "         esports_game_id       tournament_id  \\\n",
      "1343  112138727088677914  112053363288959526   \n",
      "\n",
      "                                           team_mapping  \\\n",
      "1343  {'19': '105720659034682989', '20': '1056809728...   \n",
      "\n",
      "                                    participant_mapping  \n",
      "1343  {'1': '106652148735956386', '2': '106230271915...  \n"
     ]
    }
   ],
   "source": [
    "# print vct international MAPPING_DATA_V2 with platformgame id = val:00676ae0-0a4d-4577-be1e-ab4d3a9889aa\n",
    "print (VCTInternationalDataframes[MAPPING_DATA_V2].loc[VCTInternationalDataframes[MAPPING_DATA_V2]['platform_game_id'] == 'val:00676ae0-0a4d-4577-be1e-ab4d3a9889aa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the processed game data using mapping data v2 so that it reflects the actual player and game ids\n",
    "def replace_ids_in_processed_data(processed_game_data_list, mapping_dataframe):\n",
    "    for game_data in processed_game_data_list:\n",
    "        platform_game_id = game_data.get('platformGameId', '')\n",
    "        if not platform_game_id:\n",
    "            continue  # Skip if no platformGameId\n",
    "        # Get the mapping data for this game\n",
    "        mapping_row = mapping_dataframe.loc[mapping_dataframe['platform_game_id'] == platform_game_id]\n",
    "        if mapping_row.empty:\n",
    "            continue  # No mapping data found for this game\n",
    "        # Extract the team_mapping and participant_mapping\n",
    "        mapping_row = mapping_row.iloc[0]\n",
    "        team_mapping_str = mapping_row['team_mapping']\n",
    "        participant_mapping_str = mapping_row['participant_mapping']\n",
    "        # Parse the mappings if they are strings\n",
    "        if isinstance(team_mapping_str, str):\n",
    "            try:\n",
    "                team_mapping = ast.literal_eval(team_mapping_str)\n",
    "            except:\n",
    "                team_mapping = json.loads(team_mapping_str)\n",
    "        else:\n",
    "            team_mapping = team_mapping_str\n",
    "        if isinstance(participant_mapping_str, str):\n",
    "            try:\n",
    "                participant_mapping = ast.literal_eval(participant_mapping_str)\n",
    "            except:\n",
    "                participant_mapping = json.loads(participant_mapping_str)\n",
    "        else:\n",
    "            participant_mapping = participant_mapping_str\n",
    "        # Now replace the IDs in the game_data\n",
    "        # Replace the 'winner' field\n",
    "        winner_id = game_data.get('winner')\n",
    "        if winner_id is not None:\n",
    "            winner_id_str = str(winner_id)\n",
    "            if winner_id_str in team_mapping:\n",
    "                game_data['winner'] = team_mapping[winner_id_str]\n",
    "        # Replace team IDs\n",
    "        for team_key in ['team1', 'team2']:\n",
    "            team_data = game_data.get(team_key, {})\n",
    "            team_id = team_data.get('team_id')\n",
    "            if team_id is not None:\n",
    "                team_id_str = str(team_id)\n",
    "                if team_id_str in team_mapping:\n",
    "                    team_data['team_id'] = team_mapping[team_id_str]\n",
    "            # Replace player IDs\n",
    "            players = team_data.get('players', {})\n",
    "            new_players = {}\n",
    "            for player_id_str, player_data in players.items():\n",
    "                player_id = player_data.get('player_id')\n",
    "                if player_id is not None:\n",
    "                    player_id_str = str(player_id)\n",
    "                    if player_id_str in participant_mapping:\n",
    "                        new_player_id = participant_mapping[player_id_str]\n",
    "                        # Update player_id in player_data\n",
    "                        player_data['player_id'] = new_player_id\n",
    "                        # Add to new_players with new_player_id as key\n",
    "                        new_players[new_player_id] = player_data\n",
    "                    else:\n",
    "                        # Keep the original player data\n",
    "                        new_players[player_id_str] = player_data\n",
    "                else:\n",
    "                    # No player_id, keep original\n",
    "                    new_players[player_id_str] = player_data\n",
    "            # Replace the players dictionary with new_players\n",
    "            team_data['players'] = new_players\n",
    "    return processed_game_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the ids using the mapping data v2\n",
    "VCTInternationalProcessedGamesWithIds = replace_ids_in_processed_data(VCTInternationalProcessedGames, VCTInternationalDataframes[MAPPING_DATA_V2])\n",
    "VCTChallengersProcessedGamesWithIds = replace_ids_in_processed_data(VCTChallengersProcessedGames, VCTChallengersDataframes[MAPPING_DATA_V2])\n",
    "VCTGameChangersProcessedGamesWithIds = replace_ids_in_processed_data(VCTGameChangersProcessedGames, VCTGameChangersDataframes[MAPPING_DATA_V2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VCTInternationalProcessedGamesWithIds = load_json(PROCESSED_DATA_PATHS['VCT-International'] + '/processed_game_data_with_ids.json')\n",
    "# VCTChallengersProcessedGamesWithIds = load_json(PROCESSED_DATA_PATHS['VCT-Challengers'] + '/processed_game_data_with_ids.json')\n",
    "# VCTGameChangersProcessedGamesWithIds = load_json(PROCESSED_DATA_PATHS['Game-Changers'] + '/processed_game_data_with_ids.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'winner': None, 'total_rounds': 0, 'platformGameId': 'val:00676ae0-0a4d-4577-be1e-ab4d3a9889aa'}\n"
     ]
    }
   ],
   "source": [
    "print (VCTInternationalProcessedGamesWithIds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the game data with updated ids to json files in the esports directory for each version\n",
    "def save_processed_game_data_with_id(processed_game_data, directory_path):\n",
    "    with open(os.path.join(directory_path, 'processed_game_data_with_ids.json'), 'w') as f:\n",
    "        json.dump(processed_game_data, f, indent=2)\n",
    "\n",
    "# Save the processed game data with updated IDs for each version\n",
    "save_processed_game_data_with_id(VCTInternationalProcessedGamesWithIds, PROCESSED_DATA_PATHS['VCT-International'])\n",
    "save_processed_game_data_with_id(VCTChallengersProcessedGamesWithIds, PROCESSED_DATA_PATHS['VCT-Challengers'])\n",
    "save_processed_game_data_with_id(VCTGameChangersProcessedGamesWithIds, PROCESSED_DATA_PATHS['Game-Changers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract invalid game IDs from the errors.json\n",
    "def get_invalid_game_ids(errors_data):\n",
    "    return {error[\"platform_game_id\"] for error in errors_data}\n",
    "\n",
    "errors_data = load_json(r'D:\\University of Illinois Chicago\\Classes\\Hackathon\\esportsManagerChallenge\\errors.json')\n",
    "\n",
    "invalid_game_ids = get_invalid_game_ids(errors_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out invalid games from the list of games\n",
    "def filter_invalid_games(processed_game_data, invalid_game_ids):\n",
    "    return [game for game in processed_game_data if game.get('platformGameId') not in invalid_game_ids]\n",
    "\n",
    "VCTInternationalProcessedGamesWithIdsFiltered = filter_invalid_games(VCTInternationalProcessedGamesWithIds, invalid_game_ids)\n",
    "VCTChallengersProcessedGamesWithIdsFiltered = filter_invalid_games(VCTChallengersProcessedGamesWithIds, invalid_game_ids)\n",
    "VCTGameChangersProcessedGamesWithIdsFiltered = filter_invalid_games(VCTGameChangersProcessedGamesWithIds, invalid_game_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_mapping = load_json(AGENT_MAPPING_FILE)\n",
    "agent_types_list = load_json(AGENT_TYPES_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from agent names to agent types\n",
    "agent_name_to_type = {}\n",
    "for agent_type_dict in agent_types_list:\n",
    "    for agent_type, agents in agent_type_dict.items():\n",
    "        for agent_name in agents:\n",
    "            agent_name_to_type[agent_name] = agent_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_player_stats(valid_games, agent_mapping, agent_name_to_type):\n",
    "    player_stats = {}  # Dictionary to hold aggregated stats for each player\n",
    "\n",
    "    for game in valid_games:\n",
    "        total_rounds = game.get('total_rounds', 0)\n",
    "        if total_rounds == 0:\n",
    "            continue  # Skip games with zero rounds\n",
    "\n",
    "        for team_key in ['team1', 'team2']:\n",
    "            team = game.get(team_key, {})\n",
    "            for player_id, player_data in team.get(\"players\", {}).items():\n",
    "                # Initialize player stats if not already done\n",
    "                if player_id not in player_stats:\n",
    "                    player_stats[player_id] = {\n",
    "                        \"total_games_played\": 0,\n",
    "                        \"most_played_agent_type\": None,\n",
    "                        \"most_played_agent_name\": None,  # Add this line\n",
    "                        \"average_kpr\": 0.0,\n",
    "                        \"average_dpr\": 0.0,\n",
    "                        \"total_kills_2024\": 0,\n",
    "                        \"kpr_list\": [],\n",
    "                        \"dpr_list\": [],\n",
    "                        \"headshot_ratio_list\": [],\n",
    "                        \"total_multikills\": 0,\n",
    "                        \"total_clutches\": 0,\n",
    "                        \"damage_per_round_list\": [],\n",
    "                        \"total_first_bloods\": 0  # NEW\n",
    "                    }\n",
    "\n",
    "                # Increment total games played\n",
    "                player_stats[player_id][\"total_games_played\"] += 1\n",
    "\n",
    "                # Get player's kills and deaths in this game\n",
    "                kills_in_game = player_data.get(\"kills\", 0)\n",
    "                deaths_in_game = player_data.get(\"deaths\", 0)\n",
    "\n",
    "                # Get player's multikills, clutches, and first bloods in this game\n",
    "                multikills_in_game = player_data.get(\"multiKillCount\", 0)\n",
    "                clutches_in_game = player_data.get(\"clutches\", 0)\n",
    "                first_bloods_in_game = player_data.get(\"first_bloods\", 0)  # NEW\n",
    "\n",
    "                # Add to player's totals\n",
    "                player_stats[player_id][\"total_multikills\"] += multikills_in_game\n",
    "                player_stats[player_id][\"total_clutches\"] += clutches_in_game\n",
    "                player_stats[player_id][\"total_first_bloods\"] += first_bloods_in_game  # NEW\n",
    "\n",
    "                # Compute KPR and DPR for this game\n",
    "                kpr = kills_in_game / total_rounds  # Ensure total_rounds is not zero\n",
    "                dpr = deaths_in_game / total_rounds\n",
    "\n",
    "                # Append KPR and DPR to the player's lists\n",
    "                player_stats[player_id][\"kpr_list\"].append(kpr)\n",
    "                player_stats[player_id][\"dpr_list\"].append(dpr)\n",
    "\n",
    "                # Also, store the kills for total_kills_2024\n",
    "                player_stats[player_id][\"total_kills_2024\"] += kills_in_game\n",
    "\n",
    "                # Initialize counters for headshots, total damage instances, and total damage dealt\n",
    "                total_headshots = 0\n",
    "                total_damage_instances = 0\n",
    "                total_damage_dealt = 0\n",
    "\n",
    "                # Initialize dictionaries to count agents and agent types\n",
    "                agent_name_counts = {}  # For counting agent names\n",
    "                agent_types_counts = {}  # For counting agent types\n",
    "\n",
    "                # Process per-round data to extract damage events and agent selections\n",
    "                for key, round_data in player_data.items():\n",
    "                    if key.startswith('round'):\n",
    "                        # Process damage events\n",
    "                        damage_events = round_data.get('damage_events', [])\n",
    "                        for event in damage_events:\n",
    "                            location = event.get('location', '').lower()\n",
    "                            damage_amount = event.get('damage_amount', 0)\n",
    "                            total_damage_dealt += damage_amount  # Accumulate total damage dealt\n",
    "                            # Count the damage event\n",
    "                            total_damage_instances += 1\n",
    "                            if location == 'head':\n",
    "                                total_headshots += 1\n",
    "\n",
    "                        # Process selected agent\n",
    "                        selected_agent_guid = round_data.get('selectedAgent', None)\n",
    "                        if selected_agent_guid:\n",
    "                            agent_name = agent_mapping.get(selected_agent_guid, None)\n",
    "                            if agent_name:\n",
    "                                # Count agent names\n",
    "                                agent_name_counts[agent_name] = agent_name_counts.get(agent_name, 0) + 1\n",
    "\n",
    "                                # Determine agent type\n",
    "                                agent_type = agent_name_to_type.get(agent_name, None)\n",
    "                                if agent_type:\n",
    "                                    agent_types_counts[agent_type] = agent_types_counts.get(agent_type, 0) + 1\n",
    "\n",
    "                # Compute damage per round for this game\n",
    "                if total_rounds > 0:\n",
    "                    damage_per_round = total_damage_dealt / total_rounds\n",
    "                else:\n",
    "                    damage_per_round = 0.0\n",
    "\n",
    "                # Append damage per round to the player's list\n",
    "                player_stats[player_id][\"damage_per_round_list\"].append(damage_per_round)\n",
    "\n",
    "                # Compute headshot ratio for this game\n",
    "                if total_damage_instances > 0:\n",
    "                    headshot_ratio = total_headshots / total_damage_instances\n",
    "                else:\n",
    "                    headshot_ratio = 0.0\n",
    "\n",
    "                # Append headshot ratio to the player's list\n",
    "                player_stats[player_id][\"headshot_ratio_list\"].append(headshot_ratio)\n",
    "\n",
    "                # Determine the most played agent type\n",
    "                if agent_types_counts:\n",
    "                    sorted_agent_types = sorted(agent_types_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "                    player_stats[player_id][\"most_played_agent_type\"] = sorted_agent_types[0][0]\n",
    "\n",
    "                # Determine the most played agent name\n",
    "                if agent_name_counts:\n",
    "                    sorted_agent_names = sorted(agent_name_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "                    player_stats[player_id][\"most_played_agent_name\"] = sorted_agent_names[0][0]\n",
    "\n",
    "    # After processing all games, compute averages\n",
    "    for player_id, stats in player_stats.items():\n",
    "        # Compute average KPR and DPR\n",
    "        kpr_list = stats.get(\"kpr_list\", [])\n",
    "        dpr_list = stats.get(\"dpr_list\", [])\n",
    "        if kpr_list:\n",
    "            average_kpr = sum(kpr_list) / len(kpr_list)\n",
    "        else:\n",
    "            average_kpr = 0.0\n",
    "        if dpr_list:\n",
    "            average_dpr = sum(dpr_list) / len(dpr_list)\n",
    "        else:\n",
    "            average_dpr = 0.0\n",
    "        stats[\"average_kpr\"] = average_kpr\n",
    "        stats[\"average_dpr\"] = average_dpr\n",
    "\n",
    "        # Compute average headshot ratio\n",
    "        headshot_ratio_list = stats.get(\"headshot_ratio_list\", [])\n",
    "        if headshot_ratio_list:\n",
    "            average_headshot_ratio = sum(headshot_ratio_list) / len(headshot_ratio_list)\n",
    "        else:\n",
    "            average_headshot_ratio = 0.0\n",
    "        stats[\"average_headshot_ratio\"] = average_headshot_ratio\n",
    "\n",
    "        # Compute average damage per round\n",
    "        damage_per_round_list = stats.get(\"damage_per_round_list\", [])\n",
    "        if damage_per_round_list:\n",
    "            average_damage_per_round = sum(damage_per_round_list) / len(damage_per_round_list)\n",
    "        else:\n",
    "            average_damage_per_round = 0.0\n",
    "        stats[\"average_damage_per_round\"] = average_damage_per_round\n",
    "\n",
    "        # Remove temporary lists\n",
    "        del stats['kpr_list']\n",
    "        del stats['dpr_list']\n",
    "        del stats['headshot_ratio_list']\n",
    "        del stats['damage_per_round_list']\n",
    "\n",
    "    return player_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the aggregate_player_stats function for the filtered game changer games and get player stats\n",
    "VCTInternationalPlayerStats = aggregate_player_stats(VCTInternationalProcessedGamesWithIdsFiltered, agent_mapping, agent_name_to_type)\n",
    "VCTChallengersPlayerStats = aggregate_player_stats(VCTChallengersProcessedGamesWithIdsFiltered, agent_mapping, agent_name_to_type)\n",
    "VCTGameChangersPlayerStats = aggregate_player_stats(VCTGameChangersProcessedGamesWithIdsFiltered, agent_mapping, agent_name_to_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_most_played_agent(player_stats, agent_name_to_type):\n",
    "    mismatches = []\n",
    "    for player_id, stats in player_stats.items():\n",
    "        agent_name = stats.get('most_played_agent_name')\n",
    "        agent_type = stats.get('most_played_agent_type')\n",
    "        if agent_name and agent_type:\n",
    "            # Get the agent type from the mapping\n",
    "            expected_agent_type = agent_name_to_type.get(agent_name)\n",
    "            if expected_agent_type != agent_type:\n",
    "                mismatches.append({\n",
    "                    'player_id': player_id,\n",
    "                    'agent_name': agent_name,\n",
    "                    'reported_agent_type': agent_type,\n",
    "                    'expected_agent_type': expected_agent_type\n",
    "                })\n",
    "        else:\n",
    "            print(f\"Player {player_id} is missing most played agent name or type.\")\n",
    "    return mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_most_played_agent_type(player_stats, agent_name_to_type):\n",
    "    for player_id, stats in player_stats.items():\n",
    "        agent_name = stats.get('most_played_agent_name')\n",
    "        reported_agent_type = stats.get('most_played_agent_type')\n",
    "        if agent_name and reported_agent_type:\n",
    "            # Get the expected agent type from the mapping\n",
    "            expected_agent_type = agent_name_to_type.get(agent_name)\n",
    "            if expected_agent_type != reported_agent_type:\n",
    "                # Update the most_played_agent_type to the expected agent type\n",
    "                stats['most_played_agent_type'] = expected_agent_type\n",
    "                print(f\"Updated player {player_id}'s most_played_agent_type from '{reported_agent_type}' to '{expected_agent_type}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the aggregated player stats to json files in the processed data directory for each version\n",
    "def save_aggregated_player_stats(player_stats, directory_path):\n",
    "    with open(os.path.join(directory_path, 'aggregated_player_stats.json'), 'w') as f:\n",
    "        json.dump(player_stats, f, indent=2)\n",
    "\n",
    "# Save the aggregated player stats for each version\n",
    "save_aggregated_player_stats(VCTInternationalPlayerStats, PROCESSED_DATA_PATHS['VCT-International'])\n",
    "save_aggregated_player_stats(VCTChallengersPlayerStats, PROCESSED_DATA_PATHS['VCT-Challengers'])\n",
    "save_aggregated_player_stats(VCTGameChangersPlayerStats, PROCESSED_DATA_PATHS['Game-Changers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_PLAYER_DATA = r'D:\\University of Illinois Chicago\\Classes\\Hackathon\\esportsManagerChallenge\\final_player_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(FINAL_PLAYER_DATA, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_save_player_data(player_df, player_stats_dict, category_name, teams_df, leagues_df):\n",
    "    # Convert player stats dictionary to DataFrame\n",
    "    stats_df = pd.DataFrame.from_dict(player_stats_dict, orient='index')\n",
    "    stats_df.index.name = 'player_id'\n",
    "    stats_df.reset_index(inplace=True)\n",
    "\n",
    "    # Ensure player_id is of type string in all DataFrames\n",
    "    stats_df['player_id'] = stats_df['player_id'].astype(str)\n",
    "    player_df['player_id'] = player_df['player_id'].astype(str)\n",
    "\n",
    "    # Select relevant columns from player_df\n",
    "    player_info_df = player_df[['player_id', 'handle', 'first_name', 'last_name', 'team_id']]\n",
    "\n",
    "    # Drop duplicates to prevent multiple entries per player_id and team_id\n",
    "    player_info_df = player_info_df.drop_duplicates()\n",
    "\n",
    "    # Group team_ids per player_id and aggregate player info\n",
    "    player_info_grouped = player_info_df.groupby('player_id').agg({\n",
    "        'handle': 'first',          # Assuming handle is consistent per player_id\n",
    "        'first_name': 'first',      # Assuming first_name is consistent per player_id\n",
    "        'last_name': 'first',       # Assuming last_name is consistent per player_id\n",
    "        'team_id': lambda x: list(set(x))  # Collect unique team_ids as a list\n",
    "    }).reset_index()\n",
    "\n",
    "    # Merge the DataFrames on player_id\n",
    "    merged_df = pd.merge(stats_df, player_info_grouped, on='player_id', how='left')\n",
    "\n",
    "    # Reorder columns to place player details first\n",
    "    player_details = ['player_id', 'handle', 'first_name', 'last_name']\n",
    "    stats_columns = [col for col in merged_df.columns if col not in player_details]\n",
    "    merged_df = merged_df[player_details + stats_columns]\n",
    "\n",
    "    # Explode the 'team_id' column to have one team_id per row\n",
    "    merged_df_exploded = merged_df.explode('team_id')\n",
    "\n",
    "    # Ensure team_id is of type string in both DataFrames\n",
    "    merged_df_exploded['team_id'] = merged_df_exploded['team_id'].astype(str)\n",
    "    teams_df['team_id'] = teams_df['team_id'].astype(str)\n",
    "\n",
    "    # Merge on 'team_id' to get 'league_id's\n",
    "    merged_df_exploded = pd.merge(merged_df_exploded, teams_df[['team_id', 'league_id']], on='team_id', how='left')\n",
    "\n",
    "    # Ensure 'league_id's are lists\n",
    "    merged_df_exploded['league_id'] = merged_df_exploded['league_id'].apply(\n",
    "        lambda x: x if isinstance(x, list) else [x] if pd.notnull(x) else []\n",
    "    )\n",
    "\n",
    "    # Explode the 'league_id' column to have one league_id per row\n",
    "    merged_df_exploded = merged_df_exploded.explode('league_id')\n",
    "\n",
    "    # Ensure league_id is of type string\n",
    "    merged_df_exploded['league_id'] = merged_df_exploded['league_id'].astype(str)\n",
    "    leagues_df['league_id'] = leagues_df['league_id'].astype(str)\n",
    "\n",
    "    # Merge with leagues_df to get 'region'\n",
    "    merged_df_exploded = pd.merge(merged_df_exploded, leagues_df[['league_id', 'region']], on='league_id', how='left')\n",
    "\n",
    "    # Fill missing regions with 'Unknown'\n",
    "    merged_df_exploded['region'] = merged_df_exploded['region'].fillna('Unknown')\n",
    "\n",
    "    # Group by 'player_id' to collect unique regions\n",
    "    player_regions = merged_df_exploded.groupby('player_id')['region'].apply(\n",
    "        lambda x: ', '.join(sorted(set(filter(pd.notnull, x))))\n",
    "    ).reset_index()\n",
    "\n",
    "    # Merge the regions back into the original merged_df\n",
    "    merged_df = pd.merge(merged_df, player_regions, on='player_id', how='left')\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    merged_df = merged_df.drop(columns=['team_id'])\n",
    "\n",
    "    # Round numerical columns to 3 decimal places\n",
    "    numerical_cols = merged_df.select_dtypes(include=['float', 'int']).columns\n",
    "    merged_df[numerical_cols] = merged_df[numerical_cols].round(3)\n",
    "\n",
    "    # Define the output file path\n",
    "    output_file = os.path.join(FINAL_PLAYER_DATA, f'{category_name}_PlayerData.csv')\n",
    "\n",
    "    # Save the merged DataFrame to CSV\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "    print(f\"Saved {category_name} player data to {output_file}\")\n",
    "\n",
    "    # Check for duplicates in merged_df\n",
    "    num_duplicates = merged_df['player_id'].duplicated().sum()\n",
    "    print(f\"Number of duplicate player_ids in final merged_df: {num_duplicates}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved VCT-International player data to D:\\University of Illinois Chicago\\Classes\\Hackathon\\esportsManagerChallenge\\final_player_data\\VCT-International_PlayerData.csv\n",
      "Number of duplicate player_ids in final merged_df: 0\n",
      "Saved VCT-Challengers player data to D:\\University of Illinois Chicago\\Classes\\Hackathon\\esportsManagerChallenge\\final_player_data\\VCT-Challengers_PlayerData.csv\n",
      "Number of duplicate player_ids in final merged_df: 0\n",
      "Saved Game-Changers player data to D:\\University of Illinois Chicago\\Classes\\Hackathon\\esportsManagerChallenge\\final_player_data\\Game-Changers_PlayerData.csv\n",
      "Number of duplicate player_ids in final merged_df: 0\n"
     ]
    }
   ],
   "source": [
    "# VCT International\n",
    "merge_and_save_player_data(\n",
    "    VCTInternationalDataframes[PLAYERS],\n",
    "    VCTInternationalPlayerStats,\n",
    "    'VCT-International',\n",
    "    VCTInternationalDataframes[TEAMS],\n",
    "    VCTInternationalDataframes[LEAGUES]\n",
    ")\n",
    "\n",
    "# VCT Challengers\n",
    "merge_and_save_player_data(\n",
    "    VCTChallengersDataframes[PLAYERS],\n",
    "    VCTChallengersPlayerStats,\n",
    "    'VCT-Challengers',\n",
    "    VCTChallengersDataframes[TEAMS],\n",
    "    VCTChallengersDataframes[LEAGUES]\n",
    ")\n",
    "\n",
    "# Game Changers\n",
    "merge_and_save_player_data(\n",
    "    VCTGameChangersDataframes[PLAYERS],\n",
    "    VCTGameChangersPlayerStats,\n",
    "    'Game-Changers',\n",
    "    VCTGameChangersDataframes[TEAMS],\n",
    "    VCTGameChangersDataframes[LEAGUES]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
